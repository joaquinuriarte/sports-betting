{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1c2254",
   "metadata": {},
   "source": [
    "Load Train, Val, and Test Datasets into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77a23c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_dataset leaded to memory\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Manually add the project root to sys.path\n",
    "sys.path.append('/Users/joaquinuriarte/Documents/GitHub/sports-betting/')\n",
    "\n",
    "# File path to processed_dataset\n",
    "processed_dataset_path = \"/Users/joaquinuriarte/Documents/GitHub/sports-betting/processed_datasets/model_v0/processed_dataset.pkl\"\n",
    "\n",
    "with open(processed_dataset_path, \"rb\") as f:\n",
    "    processed_dataset = pickle.load(f)\n",
    "    print(\"processed_dataset leaded to memory\")\n",
    "\n",
    "final_features = processed_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89be9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    col.startswith((\"A_\", \"B_\")) or col == \"Team_A_Wins\"\n",
    "    for col in final_features.columns\n",
    "), \"Unexpected columns found in the dataset.\"\n",
    "assert final_features[\"Team_A_Wins\"].dtype == \"int\", \"Team_A_Wins must be integer.\"\n",
    "\n",
    "assert not final_features.isna().any().any(), \"Dataset contains missing values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61c3a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    abs(final_features.filter(like=\"A_\").mean().mean() - final_features.filter(like=\"B_\").mean().mean())\n",
    "    < .0449\n",
    "), \"Team A and Team B feature distributions are significantly different.\"\n",
    "min_columns = [col for col in final_features.columns if col.endswith(\"_MIN\")]\n",
    "assert final_features[min_columns].min().min() >= 0, \"Negative MIN values detected.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "248aff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, normaltest, skew, chi2_contingency\n",
    "from typing import Dict, Any\n",
    "\n",
    "def explore_features(dataframe: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Analyze each feature in the DataFrame and print out its traits.\n",
    "\n",
    "    For this EDA, averages the features for Team A and Team B to reduce the number of features.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Input DataFrame with player-level features.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary containing analysis results for each feature.\n",
    "    \"\"\"\n",
    "    # Aggregate player-level features into team-level averages\n",
    "    team_a_features = dataframe.filter(like='A_player').groupby(lambda x: x.split('_', 3)[-1], axis=1).mean()\n",
    "    team_b_features = dataframe.filter(like='B_player').groupby(lambda x: x.split('_', 3)[-1], axis=1).mean()\n",
    "\n",
    "    # Combine aggregated features with the label\n",
    "    aggregated_dataframe = pd.concat([\n",
    "        team_a_features.add_prefix('A_'),\n",
    "        team_b_features.add_prefix('B_'),\n",
    "        dataframe[['Team_A_Wins']]\n",
    "    ], axis=1)\n",
    "\n",
    "    feature_analysis = {}\n",
    "\n",
    "    for column in aggregated_dataframe.columns:\n",
    "        if column == 'Team_A_Wins':\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nAnalyzing feature: {column}\")\n",
    "        feature_data = aggregated_dataframe[column]\n",
    "\n",
    "        # Initialize results for this feature\n",
    "        analysis = {}\n",
    "\n",
    "        # Identify the type\n",
    "        if feature_data.dtypes == 'object':\n",
    "            unique_values = feature_data.nunique()\n",
    "            if unique_values < 0.1 * len(feature_data):\n",
    "                feature_type = \"Categorical\"\n",
    "            else:\n",
    "                feature_type = \"Text\"\n",
    "        elif np.issubdtype(feature_data.dtypes, np.number):\n",
    "            feature_type = \"Numerical\"\n",
    "        else:\n",
    "            feature_type = \"Structured\"\n",
    "\n",
    "        analysis['type'] = feature_type\n",
    "\n",
    "        # Percentage of values equal to 0\n",
    "        zero_percentage = (feature_data == 0).mean() * 100\n",
    "        analysis['zero_percentage'] = zero_percentage\n",
    "\n",
    "        # Noisiness and type of noise\n",
    "        if feature_type == \"Numerical\":\n",
    "            # Outliers using IQR\n",
    "            q1, q3 = np.percentile(feature_data.dropna(), [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            outlier_percentage = ((feature_data < lower_bound) | (feature_data > upper_bound)).mean() * 100\n",
    "            analysis['outliers_percentage'] = outlier_percentage\n",
    "            analysis['noise_type'] = \"Outliers\" if outlier_percentage > 2 else \"Minimal\"\n",
    "\n",
    "        # Distribution type\n",
    "        if feature_type == \"Numerical\":\n",
    "            skewness = skew(feature_data.dropna())\n",
    "            analysis['skewness'] = skewness\n",
    "\n",
    "            # normaltest test for normality\n",
    "            p_value = normaltest(feature_data.dropna())[1]\n",
    "            if p_value > 0.05:\n",
    "                distribution_type = \"Gaussian\"\n",
    "            else:\n",
    "                distribution_type = \"Non-Gaussian\"\n",
    "\n",
    "            analysis['distribution_type'] = distribution_type\n",
    "\n",
    "        # Graph the attribute\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        if feature_type == \"Numerical\":\n",
    "            sns.histplot(feature_data, kde=True, bins=30, color='blue')\n",
    "            plt.title(f\"Distribution of {column}\")\n",
    "        elif feature_type == \"Categorical\":\n",
    "            sns.countplot(x=feature_data, palette='viridis')\n",
    "            plt.title(f\"Value Counts for {column}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Correlation metric\n",
    "        if feature_type == \"Numerical\":\n",
    "            correlation_with_others = aggregated_dataframe.corrwith(feature_data)\n",
    "            analysis['correlation_with_others'] = correlation_with_others.sort_values(ascending=False).to_dict()\n",
    "        elif feature_type == \"Categorical\":\n",
    "            # CramÃ©r's V for categorical correlation\n",
    "            def cramers_v(x, y):\n",
    "                contingency_table = pd.crosstab(x, y)\n",
    "                chi2 = chi2_contingency(contingency_table)[0]\n",
    "                n = contingency_table.sum().sum()\n",
    "                return np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "\n",
    "            categorical_cols = aggregated_dataframe.select_dtypes(include=['object']).columns\n",
    "            cramer_v_scores = {}\n",
    "            for cat_col in categorical_cols:\n",
    "                if cat_col != column:\n",
    "                    cramer_v_scores[cat_col] = cramers_v(feature_data, aggregated_dataframe[cat_col])\n",
    "            analysis['correlation_with_others'] = cramer_v_scores\n",
    "\n",
    "        feature_analysis[column] = analysis\n",
    "\n",
    "        # Print analysis\n",
    "        for key, value in analysis.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"  {key}: {{...}} (dictionary of length {len(value)})\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "    return feature_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_analysis = explore_features(final_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e14aa2",
   "metadata": {},
   "source": [
    "Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b4fe7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation analysis: printing feature pairs with more than 0.8 correlation coefficient.\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Correlation analysis: printing feature pairs with more than 0.8 correlation coefficient.\")\n",
    "# Use a set of frozensets to store symmetric pairs without duplication\n",
    "correlations = set()\n",
    "\n",
    "for columns in feature_analysis.keys():\n",
    "    for correlation in feature_analysis[columns][\"correlation_with_others\"].keys():\n",
    "        if feature_analysis[columns][\"correlation_with_others\"][correlation] >= 0.8 and columns != correlation:\n",
    "            # Add the pair as a frozenset (unordered, avoids duplicates)\n",
    "            correlations.add(frozenset([columns, correlation]))\n",
    "        \n",
    "\n",
    "for pair in correlations:\n",
    "    print(pair)\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eca1c7",
   "metadata": {},
   "source": [
    "Manual Verification of Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb54cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract the row where GAME_ID equals 52100111\n",
    "def save_row_to_excel(dataframe, game_id, output_file):\n",
    "    try:\n",
    "        # Extract the row based on the GAME_ID\n",
    "        row = dataframe.loc[game_id]\n",
    "        \n",
    "        # Check if the row exists\n",
    "        if row.empty:\n",
    "            raise ValueError(f\"No row found for GAME_ID {game_id}.\")\n",
    "        \n",
    "        # Write the row to the specified Excel file\n",
    "        row.to_excel(output_file, index=False)\n",
    "        print(f\"Row with GAME_ID {game_id} has been saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "save_row_to_excel(final_features, 52100111, '/Users/joaquinuriarte/Desktop/example_feature.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1552430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2p/36z9t47958qcdgnlc5ycsmtc0000gn/T/ipykernel_37746/1578780556.py:4: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_gd = pd.read_csv('/Users/joaquinuriarte/Desktop/dataset/games_details.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load two Excel files into DataFrames\n",
    "df_gd = pd.read_csv('/Users/joaquinuriarte/Desktop/dataset/games_details.csv')\n",
    "df_games = pd.read_csv('/Users/joaquinuriarte/Desktop/dataset/games.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "acf49582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26651\n",
      "25445\n"
     ]
    }
   ],
   "source": [
    "# Get games \n",
    "#df_gd.loc[df_gd[\"GAME_ID\"] == 52100111]\n",
    "\n",
    "print(len(df_games))\n",
    "print(len(final_features))\n",
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('sports-betting': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "42d1e72f09edad5b8341e1a78b673e0aa6929e61770b700654f70887e86e86ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
