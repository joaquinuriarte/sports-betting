{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1c2254",
   "metadata": {},
   "source": [
    "Load Train, Val, and Test Datasets into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77a23c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_dataset leaded to memory\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Manually add the project root to sys.path\n",
    "sys.path.append('/Users/joaquinuriarte/Documents/GitHub/sports-betting/')\n",
    "\n",
    "# File path to processed_dataset\n",
    "processed_dataset_path = \"/Users/joaquinuriarte/Documents/GitHub/sports-betting/processed_datasets/model_v0/processed_dataset.pkl\"\n",
    "\n",
    "with open(processed_dataset_path, \"rb\") as f:\n",
    "    processed_dataset = pickle.load(f)\n",
    "    print(\"processed_dataset leaded to memory\")\n",
    "\n",
    "final_features = processed_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AST</th>\n",
       "      <th>DREB</th>\n",
       "      <th>FG3_PCT</th>\n",
       "      <th>FG_PCT</th>\n",
       "      <th>FT_PCT</th>\n",
       "      <th>MIN</th>\n",
       "      <th>OREB</th>\n",
       "      <th>PF</th>\n",
       "      <th>PLUS_MINUS</th>\n",
       "      <th>PTS</th>\n",
       "      <th>TO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25445.000000</td>\n",
       "      <td>25445.000000</td>\n",
       "      <td>25445.000000</td>\n",
       "      <td>25445.000000</td>\n",
       "      <td>25445.000000</td>\n",
       "      <td>25445.000000</td>\n",
       "      <td>25445.000000</td>\n",
       "      <td>25445.000000</td>\n",
       "      <td>25445.000000</td>\n",
       "      <td>25445.000000</td>\n",
       "      <td>25445.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.460324</td>\n",
       "      <td>3.421917</td>\n",
       "      <td>0.228702</td>\n",
       "      <td>0.442465</td>\n",
       "      <td>0.489730</td>\n",
       "      <td>26.265257</td>\n",
       "      <td>1.135670</td>\n",
       "      <td>2.202695</td>\n",
       "      <td>0.220218</td>\n",
       "      <td>11.253220</td>\n",
       "      <td>1.487026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.678660</td>\n",
       "      <td>0.798137</td>\n",
       "      <td>0.103699</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.144603</td>\n",
       "      <td>3.564028</td>\n",
       "      <td>0.490646</td>\n",
       "      <td>0.563204</td>\n",
       "      <td>7.174650</td>\n",
       "      <td>2.205059</td>\n",
       "      <td>0.480776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.235417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-39.000000</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.154250</td>\n",
       "      <td>0.391625</td>\n",
       "      <td>0.389625</td>\n",
       "      <td>24.204167</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>-4.875000</td>\n",
       "      <td>9.875000</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>0.219500</td>\n",
       "      <td>0.442625</td>\n",
       "      <td>0.489625</td>\n",
       "      <td>27.256250</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>11.437500</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.875000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>0.493125</td>\n",
       "      <td>0.589625</td>\n",
       "      <td>28.681250</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>5.437500</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.625000</td>\n",
       "      <td>0.854750</td>\n",
       "      <td>0.785250</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>40.766667</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>34.625000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>3.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AST          DREB       FG3_PCT        FG_PCT        FT_PCT  \\\n",
       "count  25445.000000  25445.000000  25445.000000  25445.000000  25445.000000   \n",
       "mean       2.460324      3.421917      0.228702      0.442465      0.489730   \n",
       "std        0.678660      0.798137      0.103699      0.076389      0.144603   \n",
       "min        0.250000      0.625000      0.000000      0.141875      0.000000   \n",
       "25%        2.000000      2.875000      0.154250      0.391625      0.389625   \n",
       "50%        2.500000      3.437500      0.219500      0.442625      0.489625   \n",
       "75%        2.875000      4.000000      0.296875      0.493125      0.589625   \n",
       "max        6.000000      6.625000      0.854750      0.785250      0.984375   \n",
       "\n",
       "                MIN          OREB            PF    PLUS_MINUS           PTS  \\\n",
       "count  25445.000000  25445.000000  25445.000000  25445.000000  25445.000000   \n",
       "mean      26.265257      1.135670      2.202695      0.220218     11.253220   \n",
       "std        3.564028      0.490646      0.563204      7.174650      2.205059   \n",
       "min       10.235417      0.000000      0.375000    -39.000000      2.625000   \n",
       "25%       24.204167      0.750000      1.812500     -4.875000      9.875000   \n",
       "50%       27.256250      1.125000      2.187500      0.375000     11.437500   \n",
       "75%       28.681250      1.437500      2.625000      5.437500     12.750000   \n",
       "max       40.766667      3.437500      4.750000     34.625000     20.500000   \n",
       "\n",
       "                 TO  \n",
       "count  25445.000000  \n",
       "mean       1.487026  \n",
       "std        0.480776  \n",
       "min        0.000000  \n",
       "25%        1.125000  \n",
       "50%        1.500000  \n",
       "75%        1.750000  \n",
       "max        3.625000  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_a_features = final_features.filter(like='A_player').groupby(lambda x: x.split('_', 3)[-1], axis=1).mean()\n",
    "team_b_features = final_features.filter(like='B_player').groupby(lambda x: x.split('_', 3)[-1], axis=1).mean()\n",
    "team_a_features.describe()\n",
    "team_b_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89be9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    col.startswith((\"A_\", \"B_\")) or col == \"Team_A_Wins\"\n",
    "    for col in final_features.columns\n",
    "), \"Unexpected columns found in the dataset.\"\n",
    "assert final_features[\"Team_A_Wins\"].dtype == \"int\", \"Team_A_Wins must be integer.\"\n",
    "\n",
    "assert not final_features.isna().any().any(), \"Dataset contains missing values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61c3a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    abs(final_features.filter(like=\"A_\").mean().mean() - final_features.filter(like=\"B_\").mean().mean())\n",
    "    < .0449\n",
    "), \"Team A and Team B feature distributions are significantly different.\"\n",
    "min_columns = [col for col in final_features.columns if col.endswith(\"_MIN\")]\n",
    "assert final_features[min_columns].min().min() >= 0, \"Negative MIN values detected.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "248aff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, normaltest, skew, chi2_contingency\n",
    "from typing import Dict, Any\n",
    "\n",
    "def explore_features(dataframe: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Analyze each feature in the DataFrame and print out its traits.\n",
    "\n",
    "    For this EDA, averages the features for Team A and Team B to reduce the number of features.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Input DataFrame with player-level features.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary containing analysis results for each feature.\n",
    "    \"\"\"\n",
    "    # Aggregate player-level features into team-level averages\n",
    "    team_a_features = dataframe.filter(like='A_player').groupby(lambda x: x.split('_', 3)[-1], axis=1).mean()\n",
    "    team_b_features = dataframe.filter(like='B_player').groupby(lambda x: x.split('_', 3)[-1], axis=1).mean()\n",
    "\n",
    "    # Combine aggregated features with the label\n",
    "    aggregated_dataframe = pd.concat([\n",
    "        team_a_features.add_prefix('A_'),\n",
    "        team_b_features.add_prefix('B_'),\n",
    "        dataframe[['Team_A_Wins']]\n",
    "    ], axis=1)\n",
    "\n",
    "    feature_analysis = {}\n",
    "\n",
    "    for column in aggregated_dataframe.columns:\n",
    "        if column == 'Team_A_Wins':\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nAnalyzing feature: {column}\")\n",
    "        feature_data = aggregated_dataframe[column]\n",
    "\n",
    "        # Initialize results for this feature\n",
    "        analysis = {}\n",
    "\n",
    "        # Identify the type\n",
    "        if feature_data.dtypes == 'object':\n",
    "            unique_values = feature_data.nunique()\n",
    "            if unique_values < 0.1 * len(feature_data):\n",
    "                feature_type = \"Categorical\"\n",
    "            else:\n",
    "                feature_type = \"Text\"\n",
    "        elif np.issubdtype(feature_data.dtypes, np.number):\n",
    "            feature_type = \"Numerical\"\n",
    "        else:\n",
    "            feature_type = \"Structured\"\n",
    "\n",
    "        analysis['type'] = feature_type\n",
    "\n",
    "        # Percentage of values equal to 0\n",
    "        zero_percentage = (feature_data == 0).mean() * 100\n",
    "        analysis['zero_percentage'] = zero_percentage\n",
    "\n",
    "        # Noisiness and type of noise\n",
    "        if feature_type == \"Numerical\":\n",
    "            # Outliers using IQR\n",
    "            q1, q3 = np.percentile(feature_data.dropna(), [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            outlier_percentage = ((feature_data < lower_bound) | (feature_data > upper_bound)).mean() * 100\n",
    "            analysis['outliers_percentage'] = outlier_percentage\n",
    "            analysis['noise_type'] = \"Outliers\" if outlier_percentage > 2 else \"Minimal\"\n",
    "\n",
    "        # Distribution type\n",
    "        if feature_type == \"Numerical\":\n",
    "            skewness = skew(feature_data.dropna())\n",
    "            analysis['skewness'] = skewness\n",
    "\n",
    "            # normaltest test for normality\n",
    "            p_value = normaltest(feature_data.dropna())[1]\n",
    "            if p_value > 0.05:\n",
    "                distribution_type = \"Gaussian\"\n",
    "            else:\n",
    "                distribution_type = \"Non-Gaussian\"\n",
    "\n",
    "            analysis['distribution_type'] = distribution_type\n",
    "\n",
    "        # Graph the attribute\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        if feature_type == \"Numerical\":\n",
    "            sns.histplot(feature_data, kde=True, bins=30, color='blue')\n",
    "            plt.title(f\"Distribution of {column}\")\n",
    "        elif feature_type == \"Categorical\":\n",
    "            sns.countplot(x=feature_data, palette='viridis')\n",
    "            plt.title(f\"Value Counts for {column}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Correlation metric\n",
    "        if feature_type == \"Numerical\":\n",
    "            correlation_with_others = aggregated_dataframe.corrwith(feature_data)\n",
    "            analysis['correlation_with_others'] = correlation_with_others.sort_values(ascending=False).to_dict()\n",
    "        elif feature_type == \"Categorical\":\n",
    "            # Cramér's V for categorical correlation\n",
    "            def cramers_v(x, y):\n",
    "                contingency_table = pd.crosstab(x, y)\n",
    "                chi2 = chi2_contingency(contingency_table)[0]\n",
    "                n = contingency_table.sum().sum()\n",
    "                return np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "\n",
    "            categorical_cols = aggregated_dataframe.select_dtypes(include=['object']).columns\n",
    "            cramer_v_scores = {}\n",
    "            for cat_col in categorical_cols:\n",
    "                if cat_col != column:\n",
    "                    cramer_v_scores[cat_col] = cramers_v(feature_data, aggregated_dataframe[cat_col])\n",
    "            analysis['correlation_with_others'] = cramer_v_scores\n",
    "\n",
    "        feature_analysis[column] = analysis\n",
    "\n",
    "        # Print analysis\n",
    "        for key, value in analysis.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"  {key}: {{...}} (dictionary of length {len(value)})\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "    return feature_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_analysis = explore_features(final_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e14aa2",
   "metadata": {},
   "source": [
    "Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b4fe7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation analysis: printing feature pairs with more than 0.8 correlation coefficient.\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Correlation analysis: printing feature pairs with more than 0.8 correlation coefficient.\")\n",
    "# Use a set of frozensets to store symmetric pairs without duplication\n",
    "correlations = set()\n",
    "\n",
    "for columns in feature_analysis.keys():\n",
    "    for correlation in feature_analysis[columns][\"correlation_with_others\"].keys():\n",
    "        if feature_analysis[columns][\"correlation_with_others\"][correlation] >= 0.8 and columns != correlation:\n",
    "            # Add the pair as a frozenset (unordered, avoids duplicates)\n",
    "            correlations.add(frozenset([columns, correlation]))\n",
    "        \n",
    "\n",
    "for pair in correlations:\n",
    "    print(pair)\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eca1c7",
   "metadata": {},
   "source": [
    "Manual Verification of Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb54cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract the row where GAME_ID equals 52100111\n",
    "def save_row_to_excel(dataframe, game_id, output_file):\n",
    "    try:\n",
    "        # Extract the row based on the GAME_ID\n",
    "        row = dataframe.loc[game_id]\n",
    "        \n",
    "        # Check if the row exists\n",
    "        if row.empty:\n",
    "            raise ValueError(f\"No row found for GAME_ID {game_id}.\")\n",
    "        \n",
    "        # Write the row to the specified Excel file\n",
    "        row.to_excel(output_file, index=False)\n",
    "        print(f\"Row with GAME_ID {game_id} has been saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "save_row_to_excel(final_features, 52100111, '/Users/joaquinuriarte/Desktop/example_feature.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1552430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2p/36z9t47958qcdgnlc5ycsmtc0000gn/T/ipykernel_37746/1578780556.py:4: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_gd = pd.read_csv('/Users/joaquinuriarte/Desktop/dataset/games_details.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load two Excel files into DataFrames\n",
    "df_gd = pd.read_csv('/Users/joaquinuriarte/Desktop/dataset/games_details.csv')\n",
    "df_games = pd.read_csv('/Users/joaquinuriarte/Desktop/dataset/games.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "acf49582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26651\n",
      "25445\n"
     ]
    }
   ],
   "source": [
    "# Get games \n",
    "#df_gd.loc[df_gd[\"GAME_ID\"] == 52100111]\n",
    "\n",
    "print(len(df_games))\n",
    "print(len(final_features))\n",
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('sports-betting': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "42d1e72f09edad5b8341e1a78b673e0aa6929e61770b700654f70887e86e86ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
